# -*- coding: utf-8 -*-
"""Beatbox_CNN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1naJMHRQgLEvm04DbaQkHIlJkR51NU517
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

jeanthomasfc_beatbox_path = kagglehub.dataset_download('jeanthomasfc/beatbox')
jeanthomasfc_test_beatbox_path = kagglehub.dataset_download('jeanthomasfc/test-beatbox')

print('Data source import complete.')

"""Source :
- https://github.com/m3hrdadfi/soxan/blob/main/notebooks/Eating_Sound_Collection_using_Wav2Vec2.ipynb
- https://huggingface.co/docs/transformers/tasks/audio_classification

# Imports
"""

!pip install datasets
!pip install pydub
!pip install evaluate

import glob
import os
import json
import pandas as pd
import numpy as np

from pydub import AudioSegment
import torchaudio

from datasets import load_dataset
from datasets import Dataset
from datasets import ClassLabel

from sklearn.model_selection import train_test_split
import evaluate

"""# Data Collection"""

wav_files = glob.glob("/kaggle/input/beatbox/*/*/*/*.wav")
csv_files = glob.glob("/kaggle/input/beatbox/*/*/*/*.csv")
print(f"Found {len(wav_files)} .wav files and {len(csv_files)} .csv files.")

# Create a Mapping of .wav Files to .csv Files
# The .csv file has the same name as the .wav file (except the extension)
def get_filename_no_ext(path):
    return os.path.splitext(os.path.basename(path))[0]

wav_map = {get_filename_no_ext(path): path for path in wav_files}
csv_map = {get_filename_no_ext(path): path for path in csv_files}

# Ensure every .wav has a corresponding .csv
common_keys = set(wav_map.keys()) & set(csv_map.keys())
print(f"Matched {len(common_keys)} .wav files with corresponding .csv files.")

# Load Annotations and Combine with .wav File Paths
data = []
for key in common_keys:
    wav_path = wav_map[key]
    csv_path = csv_map[key]

    # Load annotations from the CSV
    annotations = pd.read_csv(csv_path, sep=",", header=None, names=["timestamp", "label"])
    # remove spaces from the anotations
    annotations["label"] = annotations["label"].str.strip()
    annotations["file_path"] = wav_path  # Add the .wav file path
    data.append(annotations)

# Combine all data into a single DataFrame
df = pd.concat(data, ignore_index=True)
print(f"Combined dataset with {len(df)} entries.")

# Create Label-to-ID Mapping
label2id = {label: i for i, label in enumerate(df["label"].unique())}
id2label = {i: label for label, i in label2id.items()}
df["label_id"] = df["label"].map(label2id)

df.info()

print("Labels: ", df["label"].unique())
print()
df.groupby("label").count()[["file_path"]]

df = df[df["label"].isin({"hhc","hho","kd", "sd"})]

print("Labels: ", df["label"].unique())
print()
df.groupby("label").count()[["file_path"]]

"""# Get Data

extract the audio from the file_path with the timestamp of the i example to the i + 1 example

If the i+1 example does not have the same file path then take the remaining of the .wav file
"""

# Function to extract audio and create manifest files
def prepare_data(df, output_folder="cnn_data", manifest_file="manifest.json"):
    os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists
    manifest = []  # List to store manifest data

    # Iterate through the rows of the DataFrame
    for i in range(len(df)):
        current_row = df.iloc[i]
        file_path = current_row['file_path']
        start_time = current_row['timestamp'] * 1000  # Convert to milliseconds

        # Determine the end time
        if i + 1 < len(df) and df.iloc[i + 1]['file_path'] == file_path:
            end_time = df.iloc[i + 1]['timestamp'] * 1000
        else:
            audio = AudioSegment.from_wav(file_path)
            end_time = len(audio)  # Length of the audio in milliseconds

        # Load and slice the audio
        audio = AudioSegment.from_wav(file_path)
        audio_segment = audio[start_time:end_time]

        # Save the extracted segment
        segment_filename = f"segment_{i}.wav"
        segment_path = os.path.join(output_folder, segment_filename)
        audio_segment.export(segment_path, format="wav")

        # Add entry to manifest
        duration = (end_time - start_time) / 1000.0  # Duration in seconds
        manifest.append({
            "audio": segment_path,
            "duration": duration,
            "label": current_row['label']
        })

    # Save the manifest file
    manifest_path = os.path.join(output_folder, manifest_file)
    with open(manifest_path, 'w') as f:
        json.dump(manifest, f, indent=4)
    print(f"Manifest file saved to: {manifest_path}")


prepare_data(df)

dataset = load_dataset("json", data_files="cnn_data/manifest.json")
df = pd.DataFrame(dataset["train"])

"""## CNN"""

# we need to distinguish the unique labels in our AVP dataset
label_list = list(df["label"].unique())
label_list.sort()  # Let's sort it for determinism
num_labels = len(label_list)
print(f"A classification problem with {num_labels} classes: {label_list}")

label2id = {label: i for i, label in enumerate(label_list)}
id2label = {i: label for i, label in enumerate(label_list)}

import cv2
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras import backend as K
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.losses import categorical_crossentropy
from tensorflow.keras.optimizers import Adadelta

import librosa
import matplotlib.pyplot as plt

!mkdir mels

def save_melspectogram(r):
  y, sr = librosa.load(r["audio"])
  mels = librosa.feature.melspectrogram(y=y, sr=sr)
  mel_path = os.path.join("mels", f'mel_{os.path.splitext(r["audio"])[0].split("/")[-1]}.png')
  r["mel"] = mel_path

  # Plot mel-spectrogram and save as image
  fig = plt.figure(figsize=(6, 4))
  plt.axis('off')  # Turn off axis for the plot
  librosa.display.specshow(librosa.power_to_db(mels, ref=np.max))
  plt.tight_layout()
  plt.savefig(mel_path)
  plt.close(fig)

  return r

df = df.apply(save_melspectogram, axis=1)

batch_size = 16
num_classes = 4
img_rows, img_cols = 100, 100

X = []
Y = []

def make_X_and_Y(r):
  x = cv2.imread(r["mel"],0).astype(np.uint8)
  x = cv2.resize(x,(img_rows,img_cols))
  X.append(x)

  y = label2id[r["label"]]
  Y.append(y)

df.apply(make_X_and_Y, axis=1)

X_array = np.array(X)
X_array_reshaped = X_array.reshape(-1,img_rows,img_cols,1)
X_array_reshaped = X_array_reshaped.astype("float32")
X_array_reshaped /= 255 # normalize pixels to have numbers between 0 and 1

Y_array = to_categorical(Y, num_classes)

x_train, x_test, y_train, y_test = train_test_split(X_array_reshaped, Y_array, test_size=0.20, random_state=42)

epochs = 20

def create_model(nb_filter_conv1=32, nb_filter_conv2=64, kernel_size=(3,3), dropout_conv=0.25, dropout_dense=0.5, optimizer='adam'):
  model = Sequential()
  model.add(Conv2D(nb_filter_conv1, kernel_size=kernel_size, activation="relu",
                  input_shape=(img_rows, img_cols, 1), padding = "valid"))
  model.add(Conv2D(nb_filter_conv2, kernel_size=kernel_size, activation="relu", padding = "valid"))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(Dropout(dropout_conv))
  model.add(Flatten())
  model.add(Dense(128, activation="relu"))
  model.add(Dropout(dropout_dense))
  model.add(Dense(num_classes, activation="softmax"))

  model.compile(loss=categorical_crossentropy,
                optimizer=optimizer,
                metrics=["accuracy"])

  return model

model = create_model()
model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))

score = model.evaluate(x_test, y_test, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

!pip install optuna
import optuna

def objective(trial):
    nb_filter_conv1 = trial.suggest_int("nb_filter_conv1", 32, 96, step=16)
    nb_filter_conv2 = trial.suggest_int("nb_filter_conv2", 32, 192, step=32)
    kernel_size = trial.suggest_int("kernel_size", 1, 9, step=2)
    dropout_conv = trial.suggest_float("dropout_conv", 0.25, 0.5, step=0.05)
    dropout_dense = trial.suggest_float("dropout_dense", 0.5, 0.75, step=0.05)
    optimizer = trial.suggest_categorical("optimizer", ["adam", "adagrad", "sgd", "nadam", "adamax"])

    model = create_model(
        nb_filter_conv1=nb_filter_conv1,
        nb_filter_conv2=nb_filter_conv2,
        kernel_size=(kernel_size,kernel_size),
        dropout_conv=dropout_conv,
        dropout_dense=dropout_dense,
        optimizer=optimizer
    )

    model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=0,
          validation_data=(x_test, y_test)
    )

    score = model.evaluate(x_test, y_test, verbose=0)
    dev_accuracy = score[1]

    return dev_accuracy

# use this function to run random search of hyperparameters, (takes a long time to run)
def search_hyperparameters(n_trials):
  # defining the random search study that will maximize the objective of the trial (val accuracy)
  # study: Optuna Study object
  study = optuna.create_study(direction="maximize", sampler=optuna.samplers.RandomSampler())

  # trying n_trials combination of hyper-parameters
  study.optimize(objective, n_trials=n_trials)

  return study

study = search_hyperparameters(n_trials=30)

"""Meilleurs hyperparamÃ¨tres: 'nb_filter_conv1': 48, 'nb_filter_conv2': 160, 'kernel_size': 7, 'dropout_conv': 0.35, 'dropout_dense': 0.7, 'optimizer': 'adamax'.

Accuracy: 0.9622833728790283.
"""

optuna.visualization.plot_optimization_history(study)

optuna.visualization.plot_parallel_coordinate(study)

"""MÃªme en changeant les hyperparmÃ¨tres, l'accuracy ne varie pas beaucoup"""

optuna.visualization.plot_param_importances(study)

"""Le kernel_size est l'hyperparamÃ¨tre qui a le plus influencÃ© l'accuracy du dev set

# Test avec deux participants

## Entrainement avec les meilleurs hyperparamÃ¨tres:
"""

def create_model(nb_filter_conv1=32, nb_filter_conv2=64, kernel_size=(3,3), dropout_conv=0.25, dropout_dense=0.5, optimizer='adam'):
  model = Sequential()
  model.add(Conv2D(nb_filter_conv1, kernel_size=kernel_size, activation="relu",
                  input_shape=(img_rows, img_cols, 1), padding = "valid"))
  model.add(Conv2D(nb_filter_conv2, kernel_size=kernel_size, activation="relu", padding = "valid"))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(Dropout(dropout_conv))
  model.add(Flatten())
  model.add(Dense(128, activation="relu"))
  model.add(Dropout(dropout_dense))
  model.add(Dense(num_classes, activation="softmax"))

  model.compile(loss=categorical_crossentropy,
                optimizer=optimizer,
                metrics=["accuracy"])

  return model

model = create_model(nb_filter_conv1=48,
                     nb_filter_conv2=160,
                     kernel_size=(7,7),
                     dropout_conv=0.35,
                     dropout_dense=0.7,
                     optimizer='adamax')

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))

score = model.evaluate(x_test, y_test, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

"""# Charger les donnÃ©es des deux participants que nous avons enregistrÃ©"""

wav_files = glob.glob("/kaggle/input/test-beatbox/*/*/*.wav")
csv_files = glob.glob("/kaggle/input/test-beatbox/*/*/*.csv")
print(f"Found {len(wav_files)} .wav files and {len(csv_files)} .csv files.")

# Create a Mapping of .wav Files to .csv Files
# The .csv file has the same name as the .wav file (except the extension)
def get_filename_no_ext(path):
    return os.path.splitext(os.path.basename(path))[0]

wav_map = {get_filename_no_ext(path): path for path in wav_files}
csv_map = {get_filename_no_ext(path): path for path in csv_files}

# Ensure every .wav has a corresponding .csv
common_keys = set(wav_map.keys()) & set(csv_map.keys())
print(f"Matched {len(common_keys)} .wav files with corresponding .csv files.")

# Load Annotations and Combine with .wav File Paths
data = []
for key in common_keys:
    wav_path = wav_map[key]
    csv_path = csv_map[key]

    # Load annotations from the CSV
    annotations = pd.read_csv(csv_path, sep=",", header=None, names=["timestamp", "label"])
    # remove spaces from the anotations
    annotations["label"] = annotations["label"].str.strip()
    annotations["file_path"] = wav_path  # Add the .wav file path
    data.append(annotations)

# Combine all data into a single DataFrame
df = pd.concat(data, ignore_index=True)
print(f"Combined dataset with {len(df)} entries.")

# Create Label-to-ID Mapping
label2id = {label: i for i, label in enumerate(df["label"].unique())}
id2label = {i: label for label, i in label2id.items()}
df["label_id"] = df["label"].map(label2id)

print("Labels: ", df["label"].unique())
print()
df.groupby("label").count()[["file_path"]]

prepare_data(df)

dataset = load_dataset("json", data_files="cnn_data/manifest.json")
df = pd.DataFrame(dataset["train"])

!mkdir mels_test

def save_melspectogram(r):
  y, sr = librosa.load(r["audio"])
  mels = librosa.feature.melspectrogram(y=y, sr=sr)
  # J'ai changÃ© le dossier d'enregistrement des fichiers mel
  mel_path = os.path.join("mels_test", f'mel_{os.path.splitext(r["audio"])[0].split("/")[-1]}.png')
  r["mel"] = mel_path

  # Plot mel-spectrogram and save as image
  fig = plt.figure(figsize=(6, 4))
  plt.axis('off')  # Turn off axis for the plot
  librosa.display.specshow(librosa.power_to_db(mels, ref=np.max))
  plt.tight_layout()
  plt.savefig(mel_path)
  plt.close(fig)

  return r

df = df.apply(save_melspectogram, axis=1)

X = []
Y = []

df.apply(make_X_and_Y, axis=1)

X_array = np.array(X)
X_array_reshaped = X_array.reshape(-1,img_rows,img_cols,1)
X_array_reshaped = X_array_reshaped.astype("float32")
X_array_reshaped /= 255 # normalize pixels to have numbers between 0 and 1

Y_array = to_categorical(Y, num_classes)

"""### Ãvaluer"""

x_test2 = X_array_reshaped
y_test2 = Y_array

personal_ds_score = model.evaluate(x_test2, y_test2, verbose=1)
print("Test loss:", personal_ds_score[0])
print("Test accuracy:", personal_ds_score[1])

"""Le modÃ¨le a l'air de bien gÃ©nÃ©raliser"""