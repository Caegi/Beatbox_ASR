# -*- coding: utf-8 -*-
"""Beatbox_Wav2Vec2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_vXlYQFde2_-ejMGyTQHqPjO7KKo4hz0

Source :
- https://github.com/m3hrdadfi/soxan/blob/main/notebooks/Eating_Sound_Collection_using_Wav2Vec2.ipynb
- https://huggingface.co/docs/transformers/tasks/audio_classification

# Imports
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install datasets
# !pip install -U flash-attn --no-build-isolation
# !pip install evaluate
# !pip install pydub
#

from google.colab import drive
import glob
import os
import json
import pandas as pd
import numpy as np


from pydub import AudioSegment
import torchaudio


from datasets import load_dataset
from datasets import Dataset
from datasets import ClassLabel

from sklearn.model_selection import train_test_split
import evaluate

from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor
from transformers import AutoConfig, AutoFeatureExtractor
from transformers import Trainer, TrainingArguments

"""# Data Collection"""

# get the dataset
drive.mount('/content/drive')

avp_dataset_path = "/content/drive/MyDrive/AVP_Dataset.zip"

# unzip
!unzip $avp_dataset_path

wav_files = glob.glob("*/*/*/*.wav")
csv_files = glob.glob("*/*/*/*.csv")
print(f"Found {len(wav_files)} .wav files and {len(csv_files)} .csv files.")

# Create a Mapping of .wav Files to .csv Files
# The .csv file has the same name as the .wav file (except the extension)
def get_filename_no_ext(path):
    return os.path.splitext(os.path.basename(path))[0]

wav_map = {get_filename_no_ext(path): path for path in wav_files}
csv_map = {get_filename_no_ext(path): path for path in csv_files}

# Ensure every .wav has a corresponding .csv
common_keys = set(wav_map.keys()) & set(csv_map.keys())
print(f"Matched {len(common_keys)} .wav files with corresponding .csv files.")

# Load Annotations and Combine with .wav File Paths
data = []
for key in common_keys:
    wav_path = wav_map[key]
    csv_path = csv_map[key]

    # Load annotations from the CSV
    annotations = pd.read_csv(csv_path, sep=",", header=None, names=["timestamp", "label"])
    # remove spaces from the anotations
    annotations["label"] = annotations["label"].str.strip()
    annotations["file_path"] = wav_path  # Add the .wav file path
    data.append(annotations)

# Combine all data into a single DataFrame
df = pd.concat(data, ignore_index=True)
print(f"Combined dataset with {len(df)} entries.")

# Create Label-to-ID Mapping
label2id = {label: i for i, label in enumerate(df["label"].unique())}
id2label = {i: label for label, i in label2id.items()}
df["label_id"] = df["label"].map(label2id)

df.info()

print("Labels: ", df["label"].unique())
print()
df.groupby("label").count()[["file_path"]]

# delete the lines with '' and 'pm' for label
if '' or 'pm' in df['label'].values:
    df = df[~df['label'].isin(['', 'pm'])]

df.groupby("label").count()[["file_path"]]

"""# Get Data

extract the audio from the file_path with the timestamp of the i example to the i + 1 example

If the i+1 example does not have the same file path then take the remaining of the .wav file
"""

# Function to extract audio and create manifest files
def prepare_wav2vec_data(df, output_folder="wav2vec_data", manifest_file="manifest.json"):
    os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists
    manifest = []  # List to store manifest data

    # Iterate through the rows of the DataFrame
    for i in range(len(df)):
        current_row = df.iloc[i]
        file_path = current_row['file_path']
        start_time = current_row['timestamp'] * 1000  # Convert to milliseconds

        # Determine the end time
        if i + 1 < len(df) and df.iloc[i + 1]['file_path'] == file_path:
            end_time = df.iloc[i + 1]['timestamp'] * 1000
        else:
            audio = AudioSegment.from_wav(file_path)
            end_time = len(audio)  # Length of the audio in milliseconds

        # Load and slice the audio
        audio = AudioSegment.from_wav(file_path)
        audio_segment = audio[start_time:end_time]

        # Save the extracted segment
        segment_filename = f"segment_{i}.wav"
        segment_path = os.path.join(output_folder, segment_filename)
        audio_segment.export(segment_path, format="wav")

        # Add entry to manifest
        duration = (end_time - start_time) / 1000.0  # Duration in seconds
        manifest.append({
            "audio": segment_path,
            "duration": duration,
            "label": current_row['label']
        })

    # Save the manifest file
    manifest_path = os.path.join(output_folder, manifest_file)
    with open(manifest_path, 'w') as f:
        json.dump(manifest, f, indent=4)
    print(f"Manifest file saved to: {manifest_path}")


prepare_wav2vec_data(df)

dataset = load_dataset("json", data_files="wav2vec_data/manifest.json")

dataset_df = pd.DataFrame(dataset["train"])

print(dataset_df)

train_dataset, test_dataset = train_test_split(dataset_df, test_size=0.2, random_state=42)

train_dataset = Dataset.from_pandas(pd.DataFrame(train_dataset))
test_dataset = Dataset.from_pandas(pd.DataFrame(test_dataset))

train_dataset.save_to_disk("train")
test_dataset.save_to_disk("test")

print(train_dataset)
print(test_dataset)

print(train_dataset[0])
print(train_dataset[1])
print(test_dataset[0])

label2id = {label: i for i, label in enumerate(dataset_df["label"].unique())}
# Create ClassLabel feature for the 'label' column
train_dataset = train_dataset.cast_column("label", ClassLabel(num_classes=len(label2id), names=list(label2id.keys())))

test_dataset = test_dataset.cast_column("label", ClassLabel(num_classes=len(label2id), names=list(label2id.keys())))

print(train_dataset[0])
print(train_dataset[1])
print(test_dataset[0])

"""# Prepare data for Training

As we don't have the sampling rate of this dataset available we will make sure it's a 16000khz, as required by Wav2Vec.
"""

# We need to specify the input and output column
input_column = "audio"
output_column = "label"

# we need to distinguish the unique labels in our AVP dataset
label_list = train_dataset.unique(output_column)
label_list.sort()  # Let's sort it for determinism
num_labels = len(label_list)
print(f"A classification problem with {num_labels} classes: {label_list}")

model_name_or_path = "facebook/wav2vec2-base"
pooling_mode = "mean"
feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)
target_sampling_rate = feature_extractor.sampling_rate
print(f"The target sampling rate: {target_sampling_rate}")

# config
config = AutoConfig.from_pretrained(
    model_name_or_path,
    num_labels=num_labels,
    label2id={label: i for i, label in enumerate(label_list)},
    id2label={i: label for i, label in enumerate(label_list)},
    finetuning_task="wav2vec2_clf",
)
setattr(config, 'pooling_mode', pooling_mode)

"""# Preprocess the data


Now, we need to extract features from the audio path in context representation tensors and feed them into our classification model to determine the beatbox sound.
Since the audio file is saved in the .wav format, it is easy to use Librosa.
An audio file usually stores both its values and the sampling rate with which the speech signal was digitalized. We want to store both in the dataset and write a map(...) function accordingly. Also, we need to handle the string labels into integers for our specific classification task in this case, the single-label classification.

--> https://github.com/m3hrdadfi/soxan/blob/main/notebooks/Eating_Sound_Collection_using_Wav2Vec2.ipynb
"""

def speech_file_to_array_fn(path, target_sampling_rate=16000):
    try:
        speech_array, sampling_rate = torchaudio.load(path)
        if sampling_rate != target_sampling_rate:
            resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)
            speech_array = resampler(speech_array)

        # Potential shape issues if the audio itself is empty/corrupted
        # Make sure `speech_array` is not shape=[0], shape=[], etc.
        speech = speech_array.squeeze().numpy()
        if speech.ndim == 0:
            # means it's a single scalar or empty
            print(f"Warning: Audio at {path} has shape={speech_array.shape}.")
            return None
        return speech
    except RuntimeError as e:
        print(f"Warning: Failed to decode audio at {path}. Error: {e}")
        return None


def label_to_id(label, label_list):

    if len(label_list) > 0:
        return label_list.index(label) if label in label_list else -1

    return label

def preprocess_function(examples):
    speech_arrays = []
    labels = []

    for path, label in zip(examples[input_column], examples[output_column]):
        speech = speech_file_to_array_fn(path, target_sampling_rate=target_sampling_rate)
        if speech is not None:
            speech_arrays.append(speech)
            labels.append(label_to_id(label, label_list))
        else:
            print(f"Skipping corrupted file: {path}")

    inputs = feature_extractor(
        speech_arrays,
        sampling_rate=target_sampling_rate,
        return_tensors="pt",
        padding=True
    )
    # Return your usual fields
    inputs["labels"] = labels
    return {"input_values": inputs.input_values, "labels": inputs["labels"]}

# Test part, to be removed for the training
# max_samples = 100
train_dataset = train_dataset #.select(range(max_samples))
test_dataset = test_dataset #.select(range(max_samples))

# def is_valid_audio(example):
#     path = example["audio"]
#     try:
#         # attempt load
#         torchaudio.load(path)
#         return True
#     except:
#         return False

# # Filter out bad files
# train_dataset = train_dataset.filter(is_valid_audio)

train_dataset = train_dataset.map(
    preprocess_function,
    batch_size=10,
    batched=True,
    num_proc=4,
    remove_columns=["audio", "duration", "label", "__index_level_0__"]
)
test_dataset = test_dataset.map(
    preprocess_function,
    batch_size=10,
    batched=True,
    num_proc=4,
    remove_columns=["audio", "duration", "label", "__index_level_0__"]
)

train_dataset

"""# Training"""

os.environ["WANDB_DISABLED"] = "true"

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    predictions = np.argmax(eval_pred.predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)

# model = Wav2Vec2ForSequenceClassification.from_pretrained(
#     model_name_or_path,
#     config=config,
# )

# training_args = TrainingArguments(
#     output_dir="/content/wav2vec2-base-beatbox",
#     eval_strategy="epoch",
#     save_strategy="epoch",
#     learning_rate=3e-5,
#     per_device_train_batch_size=4,
#     gradient_accumulation_steps=2,
#     per_device_eval_batch_size=4,
#     num_train_epochs=10,
#     warmup_ratio=0.1,
#     logging_steps=10,
#     load_best_model_at_end=True,
#     metric_for_best_model="accuracy"
# )


# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_dataset,
#     eval_dataset=test_dataset,
#     tokenizer=feature_extractor,
#     compute_metrics=compute_metrics,
# )

# trainer.train()

# TODO
# train the model with the full train / dev set
# push to the hub
# create a dev set and do the inference

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(f"Device: {device}")

# model_name_or_path = "model_path"
# config = AutoConfig.from_pretrained(model_name_or_path)
# feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name_or_path)
# model = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path).to(device)

"""# Hyperparameters Search (Random Search)

We choose to use a random search method pour search for hyperparameters. We will use `optuna` as a backend for randomizing parameters. We need to install it and do some imports
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install optuna

import optuna
from optuna.samplers import RandomSampler

"""Following the hugginface's Trainer API we have to init the model with model_init()"""

# This function is called every time we want to try a different hyperparameter configuration
def model_init():
    return Wav2Vec2ForSequenceClassification.from_pretrained(
        model_name_or_path,
        config=config,
    )

training_args = TrainingArguments(
    output_dir="/content/wav2vec2-base-beatbox",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=3e-5,  # Will be overridden by hyperparameter search
    per_device_train_batch_size=4,  # Will be overridden by hyperparameter search
    gradient_accumulation_steps=2,
    per_device_eval_batch_size=4,
    num_train_epochs=10,  # Can also be overridden
    warmup_ratio=0.1,
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy"
)

trainer = Trainer(
    model_init=model_init,                # the model_init method
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=feature_extractor,
    compute_metrics=compute_metrics,
)

"""We now need to define and hyperparameters space where our random search is restricted, we can experiment here we different combinaison"""

def hp_space(trial):
    return {
        # Random log-uniform between 1e-5 and 5e-5
        "learning_rate": trial.suggest_float("learning_rate", 1e-5, 5e-5, log=True),

        # Choice of batch sizes
        "per_device_train_batch_size": trial.suggest_categorical(
            "per_device_train_batch_size", [8, 16]
        ),

        # Possibly vary the number of epochs (maybe unrelevant)
        "num_train_epochs": trial.suggest_categorical("num_train_epochs", [10])
    }

sampler = RandomSampler()

best_run = trainer.hyperparameter_search(
    direction="maximize",
    hp_space=hp_space,
    backend="optuna",
    n_trials=3,       # how many random trials
    sampler=sampler
)

best_run